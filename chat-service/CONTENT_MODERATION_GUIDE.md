# Content Moderation System - Implementation Guide

## Overview

The content moderation system automatically detects and blocks inappropriate content (pornography, violence, drugs, offensive material) in images and videos uploaded to the status feature. Users are warned or blocked based on the severity of the content detected.

## Features

- **Automatic Detection**: AI-powered detection of inappropriate content in images and videos
- **Multi-Category Detection**: Detects nudity, violence, drugs, alcohol, weapons, and offensive content
- **Tiered Response**: Block explicit content, warn for suggestive content
- **Admin Review**: Flagged content is logged for admin review
- **Configurable Thresholds**: Adjust sensitivity levels for different content types
- **Multiple Providers**: Support for Sightengine (default), Google Cloud Vision, AWS Rekognition

---

## Content Moderation Service

### Supported Providers

#### 1. **NSFW.js** (Recommended - FREE & Open Source)
- **Cost**: **FREE** - No API costs, unlimited usage
- **Speed**: Fast (200-500ms per image on CPU)
- **Accuracy**: 90-93% accuracy for adult content
- **Features**: Nudity detection (Porn, Hentai, Sexy categories)
- **Setup**: Native Node.js library, self-hosted on your VPS
- **Source**: https://github.com/infinitered/nsfwjs
- **Deployment**: See [NSFWJS_DEPLOYMENT_GUIDE.md](./NSFWJS_DEPLOYMENT_GUIDE.md)

#### 2. **Sightengine** (Paid Alternative)
- **Cost**: Pay-as-you-go, ~$1-2 per 1000 images (~$50-200/month)
- **Speed**: Fast (< 1 second per image)
- **Accuracy**: High accuracy (95%+) for adult content
- **Features**: Nudity, violence, weapons, drugs, offensive content, granular classification
- **Setup**: Simple REST API, no SDK required
- **Sign up**: https://sightengine.com/

#### 2. **Google Cloud Vision API**
- **Cost**: $1.50 per 1000 images
- **Speed**: Fast
- **Accuracy**: Very high
- **Features**: Safe search detection, label detection
- **Setup**: Requires Google Cloud account and SDK

#### 3. **AWS Rekognition**
- **Cost**: $1.00 per 1000 images
- **Speed**: Fast
- **Accuracy**: High
- **Features**: Content moderation, unsafe content detection
- **Setup**: Requires AWS account and SDK

---

## Setup Instructions

### Step 1: Choose a Provider

We recommend **NSFW.js** for its zero cost, privacy, and ease of deployment.

**Comparison:**

| Feature | NSFW.js (FREE) | Sightengine (Paid) |
|---------|----------------|-------------------|
| Cost | **$0/month** | $50-500/month |
| Accuracy | 90-93% | 95%+ |
| Privacy | All local | Images sent to API |
| Setup | Simple (npm install) | Very simple (API key) |
| Maintenance | Self-hosted | Managed service |
| VPS Requirements | +300MB RAM | None |

**Choose NSFW.js if:**
- ✅ You want zero ongoing costs
- ✅ You value privacy (images stay on your server)
- ✅ You have a standard VPS (2-4GB RAM)
- ✅ 90-93% accuracy is sufficient

**Choose Sightengine if:**
- ✅ You need 95%+ accuracy
- ✅ You want managed service (no maintenance)
- ✅ You need granular classification (specific body parts)
- ✅ Cost is not a concern

For most use cases, **NSFW.js is the better choice**.

### Step 2A: Setup NSFW.js (FREE - Recommended)

**On your Linux VPS (production):**

```bash
cd chat-service
npm install nsfwjs @tensorflow/tfjs-node
```

That's it! No API keys needed.

**On Windows (development):**

Use Docker (TensorFlow.js requires build tools on Windows):

```bash
docker-compose up chat-service
```

### Step 2B: Setup Sightengine (Paid - Alternative)

1. Go to https://sightengine.com/
2. Create an account
3. Get your API credentials:
   - API User
   - API Secret

### Step 3: Configure Environment Variables

Add these to your `chat-service/.env` file:

**For NSFW.js (FREE):**
```bash
# Enable content moderation
CONTENT_MODERATION_ENABLED=true
CONTENT_MODERATION_PROVIDER=nsfwjs

# Moderation thresholds (0.0 to 1.0)
MODERATION_BLOCK_THRESHOLD=0.85
MODERATION_WARN_THRESHOLD=0.60
```

**For Sightengine (Paid):**
```bash
# Enable content moderation
CONTENT_MODERATION_ENABLED=true
CONTENT_MODERATION_PROVIDER=sightengine

# Sightengine API credentials
SIGHTENGINE_API_USER=your_api_user_here
SIGHTENGINE_API_SECRET=your_api_secret_here

# Moderation thresholds (0.0 to 1.0)
MODERATION_BLOCK_THRESHOLD=0.85
MODERATION_WARN_THRESHOLD=0.60
```

### Step 4: Test the Configuration

```bash
cd chat-service
npm install
npm run dev
```

Upload an image to a status and verify moderation is working.

---

## How It Works

### Flow Diagram

```
User uploads status with image/video
        ↓
Media URL generated by settings-service
        ↓
Status service calls content moderation
        ↓
Moderation service analyzes content
        ↓
    ┌───────────────┐
    │  Result:      │
    │  • Allow      │ → Status created successfully
    │  • Warn       │ → Status created, flagged for review
    │  • Block      │ → Status rejected, user warned
    └───────────────┘
```

### Moderation Thresholds

The system uses confidence scores (0.0 to 1.0) to determine actions:

| Score Range | Action | Description |
|-------------|--------|-------------|
| 0.00 - 0.59 | **Allow** | Content is appropriate |
| 0.60 - 0.84 | **Warn** | Potentially inappropriate, flagged for review |
| 0.85 - 1.00 | **Block** | Explicit content, upload rejected |

### Content Categories Checked

1. **Nudity/Sexual Content**
   - Explicit nudity
   - Suggestive content
   - Sexual activity

2. **Violence**
   - Weapons
   - Blood/gore
   - Violent acts

3. **Drugs & Alcohol**
   - Drug paraphernalia
   - Alcohol consumption
   - Smoking

4. **Offensive Content**
   - Hate symbols
   - Offensive gestures
   - Disturbing imagery

---

## API Integration

### Status Model Changes

The status model now includes moderation fields:

```typescript
interface IStatus {
    // ... existing fields

    contentModerationChecked: boolean;
    contentModerationResult?: {
        action: 'allow' | 'warn' | 'block';
        reason?: string;
        checkedAt: Date;
        scores?: {
            nudity?: number;
            suggestive?: number;
            violence?: number;
            drugs?: number;
            alcohol?: number;
            gore?: number;
        };
    };
    contentWarned: boolean;
    contentWarnedAt?: Date;
}
```

### Status Creation Flow

When creating a status with media:

```typescript
// In status.service.ts
async createStatus(data: CreateStatusData): Promise<IStatus> {
    // ... validation

    // Content moderation for images/videos
    if (mediaUrl && (isImage || isVideo)) {
        const moderationResult = await contentModerationService.moderateImage(mediaUrl);

        if (moderationResult.action === 'block') {
            throw new Error(
                'Your content has been blocked due to: ' + moderationResult.reason +
                '. Pornographic, violent, or offensive content is not allowed.'
            );
        }

        if (moderationResult.action === 'warn') {
            log.warn(`Flagged content uploaded by user ${userId}`);
            // Status is created but flagged
        }
    }

    // Create status with moderation result
    const status = await statusRepository.create({
        // ... status data
        contentModerationChecked: true,
        contentModerationResult: moderationResult,
        contentWarned: moderationResult.action === 'warn'
    });

    return status;
}
```

---

## Error Handling

### User-Facing Errors

When content is blocked:

```json
{
    "success": false,
    "message": "Your content has been blocked due to: Explicit nudity detected. Pornographic, violent, or offensive content is not allowed on this platform."
}
```

### Service Errors

If the moderation service fails:
- Content is **allowed by default** to avoid blocking legitimate content
- Error is logged for admin review
- System continues normal operation

```typescript
try {
    moderationResult = await contentModerationService.moderateImage(url);
} catch (error) {
    log.error('Moderation service error:', error);
    // Allow content (fail-open for user experience)
    return { isAppropriate: true, action: 'allow' };
}
```

---

## Admin Dashboard Integration

### View Flagged Content

Admins should be able to review flagged content through a dashboard.

**Endpoint**: `GET /api/status/admin/flagged`

```typescript
// Get statuses flagged for review
const flaggedStatuses = await Status.find({
    contentWarned: true,
    deleted: false
}).sort({ contentWarnedAt: -1 });
```

### Review Action

Admins can:
1. **Approve**: Remove warning flag
2. **Delete**: Remove the status
3. **Ban User**: Block user from posting

**Sample Response**:

```json
{
    "statuses": [
        {
            "_id": "status_id",
            "authorId": "user_id",
            "content": "Status text",
            "mediaUrl": "https://...",
            "contentModerationResult": {
                "action": "warn",
                "reason": "Suggestive content detected",
                "scores": {
                    "nudity": 0.65,
                    "suggestive": 0.72
                }
            },
            "contentWarnedAt": "2025-12-17T10:00:00Z"
        }
    ]
}
```

---

## Configuration Options

### Adjusting Thresholds

Based on your community standards, you can adjust the thresholds:

**More Strict** (block more content):
```bash
MODERATION_BLOCK_THRESHOLD=0.70  # Lower = more strict
MODERATION_WARN_THRESHOLD=0.50
```

**More Lenient** (allow more content):
```bash
MODERATION_BLOCK_THRESHOLD=0.90  # Higher = more lenient
MODERATION_WARN_THRESHOLD=0.70
```

### Disabling Moderation

For development or testing:

```bash
CONTENT_MODERATION_ENABLED=false
```

⚠️ **Warning**: Never disable moderation in production!

---

## Testing

### Manual Testing

1. **Test with Appropriate Image**:
   ```
   Upload a normal photo → Should succeed
   ```

2. **Test with Inappropriate Image**:
   ```
   Upload suggestive content → Should warn or block
   Check logs for moderation scores
   ```

3. **Test Error Handling**:
   ```
   Disable Sightengine temporarily → Upload should still work
   Check logs for error message
   ```

### Automated Testing

```typescript
// test/content-moderation.test.ts

describe('Content Moderation', () => {
    it('should allow appropriate content', async () => {
        const result = await contentModerationService.moderateImage(
            'https://example.com/appropriate.jpg'
        );
        expect(result.action).toBe('allow');
    });

    it('should block explicit content', async () => {
        // Use Sightengine test URLs for explicit content
        const result = await contentModerationService.moderateImage(
            'https://sightengine.com/assets/stream/examples/nudity1.jpg'
        );
        expect(result.action).toBe('block');
    });
});
```

---

## Monitoring & Analytics

### Log Analysis

Monitor logs for patterns:

```bash
# View blocked content attempts
grep "blocked due to" logs/chat-service.log

# View flagged content
grep "flagged with warning" logs/chat-service.log

# Count moderation actions per day
grep "contentModerationChecked" logs/chat-service.log | wc -l
```

### Metrics to Track

1. **Total moderation checks per day**
2. **Block rate** (blocked / total checks)
3. **Warn rate** (warned / total checks)
4. **User patterns** (repeat offenders)
5. **Category distribution** (what types of content are flagged)

### Database Queries

```javascript
// Get moderation statistics
const stats = await Status.aggregate([
    {
        $match: {
            contentModerationChecked: true,
            createdAt: { $gte: new Date(Date.now() - 30 * 24 * 60 * 60 * 1000) } // Last 30 days
        }
    },
    {
        $group: {
            _id: '$contentModerationResult.action',
            count: { $sum: 1 }
        }
    }
]);

// Get users with most flagged content
const topOffenders = await Status.aggregate([
    {
        $match: {
            contentWarned: true,
            deleted: false
        }
    },
    {
        $group: {
            _id: '$authorId',
            flaggedCount: { $sum: 1 }
        }
    },
    { $sort: { flaggedCount: -1 } },
    { $limit: 10 }
]);
```

---

## Cost Comparison

### NSFW.js (Self-Hosted) vs Sightengine (API)

| Volume/Month | NSFW.js Cost | Sightengine Cost | **Savings** |
|--------------|--------------|------------------|-------------|
| 10,000 images | **$0** | $15 | **$15/month** |
| 100,000 images | **$0** | $100 | **$100/month** |
| 1,000,000 images | **$0** | $500 | **$500/month** |

**NSFW.js Requirements:**
- VPS: +300MB RAM, +100MB disk
- No additional costs

**Annual Savings:**
- 100K images/month: **$1,200/year**
- 1M images/month: **$6,000/year**

### Sightengine Pricing (if you choose paid option)

| Volume | Price per 1000 images | Monthly Cost (example) |
|--------|----------------------|------------------------|
| 0 - 10K | $2.00 | $20 |
| 10K - 100K | $1.50 | $150 |
| 100K - 1M | $1.00 | $1,000 |
| 1M+ | $0.50 | Custom |

**Example Calculation**:
- 1,000 status uploads per day with images/videos
- 30,000 per month
- Cost: ~$45/month

### Cost Optimization

1. **Cache Results**: Don't re-check the same image
2. **Thumbnail First**: Check thumbnails instead of full videos
3. **Batch Processing**: Process multiple images in one request (if supported)
4. **Skip Low-Risk**: Don't check text-only statuses

---

## Troubleshooting

### Common Issues

#### 1. "Moderation service error" in logs

**Cause**: API credentials invalid or quota exceeded
**Solution**:
- Verify API credentials in .env
- Check Sightengine dashboard for quota
- Test API directly: `curl -X GET "https://api.sightengine.com/1.0/check.json?url=...&models=nudity-2.0&api_user=...&api_secret=..."`

#### 2. All content being blocked

**Cause**: Threshold too low
**Solution**: Increase `MODERATION_BLOCK_THRESHOLD` to 0.90

#### 3. Inappropriate content getting through

**Cause**: Threshold too high
**Solution**: Decrease `MODERATION_BLOCK_THRESHOLD` to 0.75 or 0.80

#### 4. Slow status creation

**Cause**: Moderation API taking too long
**Solution**:
- Check Sightengine service status
- Consider implementing async moderation (moderate after upload)
- Use thumbnail URLs for videos instead of full videos

---

## Future Enhancements

1. **User Appeals**: Let users appeal blocked content
2. **Machine Learning**: Train custom models on your community content
3. **Async Moderation**: Check content after upload, remove if inappropriate
4. **User Reputation**: Reduce checks for trusted users
5. **Regional Settings**: Different thresholds for different regions
6. **Custom Categories**: Add checks for specific content types (e.g., scams, spam)

---

## Security & Privacy

### Data Privacy

- Sightengine **does not store** uploaded images permanently
- Images are processed and discarded immediately
- No personal data is sent to moderation service
- Results are stored in your database only

### API Security

- API credentials are stored as environment variables
- Never commit API keys to version control
- Use different credentials for dev/staging/production
- Rotate API keys periodically

### User Privacy

- Users are **not** notified of specific moderation scores
- Only generic rejection messages are shown
- Admin access to flagged content should be logged
- Consider GDPR compliance for EU users

---

## Support & Resources

### Sightengine Documentation
- https://sightengine.com/docs/
- https://sightengine.com/docs/nudity-detection
- https://sightengine.com/docs/video-moderation

### Alternative Providers
- Google Cloud Vision: https://cloud.google.com/vision/docs/detecting-safe-search
- AWS Rekognition: https://docs.aws.amazon.com/rekognition/latest/dg/moderation.html

### Community Guidelines Template
Create clear community guidelines that explain:
- What content is not allowed
- How moderation works
- What happens when content is flagged
- How to appeal decisions

---

## Summary

The content moderation system:
✅ **Automatically** detects inappropriate content
✅ **Blocks** explicit pornographic/violent content
✅ **Warns** about suggestive content for admin review
✅ **Protects** your community from harmful content
✅ **Configurable** thresholds for different use cases
✅ **FREE with NSFW.js** - Zero ongoing costs
✅ **90-93% accuracy** - Comparable to paid services
✅ **Easy to integrate** with existing status upload flow

### Recommended Setup: NSFW.js

For most use cases, we recommend **NSFW.js**:
- ✅ **$0 cost** - Free forever
- ✅ **High accuracy** - 90-93% detection rate
- ✅ **Privacy-first** - All processing on your server
- ✅ **Easy deployment** - Works on Linux VPS out of the box

**Setup time**: 30 minutes
**Monthly cost**: $0
**Annual savings vs Sightengine**: $180-6,000

### Resources

- **NSFW.js Deployment**: See [NSFWJS_DEPLOYMENT_GUIDE.md](./NSFWJS_DEPLOYMENT_GUIDE.md)
- **NSFW.js GitHub**: https://github.com/infinitered/nsfwjs
- **Sightengine Docs**: https://sightengine.com/docs/

For questions or issues, contact the development team.
